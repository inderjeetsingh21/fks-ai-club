<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LoRA Fine-Tuning Visualizer - Efficient Model Adaptation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }

        .container {
            max-width: 1900px;
            margin: 0 auto;
        }

        .header {
            text-align: center;
            color: white;
            margin-bottom: 30px;
        }

        .header h1 {
            font-size: 3.5em;
            margin-bottom: 10px;
            text-shadow: 3px 3px 6px rgba(0,0,0,0.5);
        }

        .header p {
            font-size: 1.5em;
            opacity: 0.95;
        }

        .panel {
            background: white;
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.4);
            margin-bottom: 20px;
        }

        .panel h2 {
            color: #0f3460;
            margin-bottom: 20px;
            font-size: 2em;
            border-bottom: 3px solid #0f3460;
            padding-bottom: 12px;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
        }

        .comparison-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }

        .method-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 30px;
            border-radius: 15px;
            border: 3px solid transparent;
            transition: all 0.3s;
        }

        .method-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
        }

        .method-card.full-finetune {
            border-color: #e74c3c;
        }

        .method-card.lora {
            border-color: #2ecc71;
            background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
        }

        .method-card h3 {
            color: #0f3460;
            font-size: 1.8em;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .info-box {
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 5px solid #2196F3;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            line-height: 1.8;
        }

        .info-box strong {
            color: #1565C0;
            font-size: 1.1em;
        }

        .success-box {
            background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%);
            border-left: 5px solid #28a745;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .warning-box {
            background: linear-gradient(135deg, #fff3cd 0%, #ffe69c 100%);
            border-left: 5px solid #ffc107;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .stat-card {
            background: white;
            padding: 25px;
            border-radius: 12px;
            text-align: center;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .stat-card.bad {
            border: 3px solid #e74c3c;
        }

        .stat-card.good {
            border: 3px solid #2ecc71;
        }

        .stat-value {
            font-size: 2.5em;
            font-weight: bold;
            color: #0f3460;
            margin: 10px 0;
        }

        .stat-value.bad {
            color: #e74c3c;
        }

        .stat-value.good {
            color: #2ecc71;
        }

        .stat-label {
            color: #666;
            font-size: 0.95em;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
        }

        .matrix-visualization {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
        }

        .matrix-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
            margin: 20px 0;
        }

        .matrix {
            background: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            position: relative;
        }

        .matrix-label {
            text-align: center;
            font-weight: bold;
            color: #0f3460;
            margin-bottom: 15px;
            font-size: 1.1em;
        }

        .matrix-grid {
            display: grid;
            gap: 2px;
            background: #e0e0e0;
            padding: 2px;
            border-radius: 6px;
        }

        .matrix-cell {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            aspect-ratio: 1;
            border-radius: 2px;
            transition: all 0.3s;
        }

        .matrix-cell:hover {
            transform: scale(1.1);
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }

        .matrix-cell.trainable {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            animation: pulse 2s infinite;
        }

        .matrix-cell.frozen {
            background: linear-gradient(135deg, #a8a8a8 0%, #6e6e6e 100%);
        }

        .matrix-cell.lora-adapter {
            background: linear-gradient(135deg, #2ecc71 0%, #27ae60 100%);
            animation: glow 2s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.6; }
        }

        @keyframes glow {
            0%, 100% { box-shadow: 0 0 5px rgba(46, 204, 113, 0.5); }
            50% { box-shadow: 0 0 20px rgba(46, 204, 113, 1); }
        }

        .matrix-dimensions {
            text-align: center;
            margin-top: 10px;
            color: #666;
            font-size: 0.9em;
        }

        .operator {
            font-size: 2em;
            color: #0f3460;
            font-weight: bold;
        }

        .equals {
            font-size: 2em;
            color: #2ecc71;
            font-weight: bold;
        }

        .formula-box {
            background: #2d2d2d;
            color: #00ff00;
            padding: 25px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            font-size: 1.1em;
            line-height: 2;
        }

        .formula-box .formula-title {
            color: #ffd700;
            font-weight: bold;
            margin-bottom: 15px;
            font-size: 1.2em;
        }

        .formula-box .variable {
            color: #00ffff;
        }

        .formula-box .operator {
            color: #ff69b4;
            font-size: 1em;
        }

        .btn-primary {
            padding: 18px 40px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 10px;
            font-size: 1.2em;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s;
            margin: 10px 5px;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.4);
        }

        .btn-primary:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .btn-secondary {
            padding: 18px 40px;
            background: linear-gradient(135deg, #95a5a6 0%, #7f8c8d 100%);
            color: white;
            border: none;
            border-radius: 10px;
            font-size: 1.2em;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s;
            margin: 10px 5px;
        }

        .btn-secondary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(127, 140, 141, 0.4);
        }

        .training-progress {
            margin: 30px 0;
        }

        .progress-bar-container {
            width: 100%;
            height: 40px;
            background: #e0e0e0;
            border-radius: 20px;
            overflow: hidden;
            position: relative;
            margin: 15px 0;
        }

        .progress-bar {
            height: 100%;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transition: width 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .progress-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 10px;
            font-weight: bold;
        }

        .training-log {
            background: #2d2d2d;
            color: #00ff00;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            max-height: 300px;
            overflow-y: auto;
            margin: 20px 0;
        }

        .log-entry {
            margin-bottom: 5px;
            animation: fadeIn 0.5s;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 15px;
            text-align: left;
            border-bottom: 2px solid #e0e0e0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #0f3460 0%, #1a1a2e 100%);
            color: white;
            font-weight: bold;
        }

        .comparison-table tr:hover {
            background: #f8f9fa;
        }

        .comparison-table .good {
            color: #2ecc71;
            font-weight: bold;
        }

        .comparison-table .bad {
            color: #e74c3c;
            font-weight: bold;
        }

        .rank-selector {
            margin: 20px 0;
        }

        .rank-selector label {
            display: block;
            margin-bottom: 10px;
            font-weight: bold;
            color: #333;
            font-size: 1.1em;
        }

        .rank-slider {
            width: 100%;
            height: 8px;
            border-radius: 4px;
            background: #e0e0e0;
            outline: none;
            -webkit-appearance: none;
        }

        .rank-slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            background: #2ecc71;
            cursor: pointer;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
        }

        .rank-slider::-moz-range-thumb {
            width: 24px;
            height: 24px;
            border-radius: 50%;
            background: #2ecc71;
            cursor: pointer;
            box-shadow: 0 2px 8px rgba(0,0,0,0.3);
            border: none;
        }

        .rank-value {
            text-align: center;
            font-size: 2em;
            font-weight: bold;
            color: #2ecc71;
            margin: 10px 0;
        }

        .use-case-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .use-case-card {
            background: white;
            padding: 25px;
            border-radius: 12px;
            border-left: 5px solid #667eea;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            transition: all 0.3s;
        }

        .use-case-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 20px rgba(0,0,0,0.15);
        }

        .use-case-card h4 {
            color: #0f3460;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .architecture-diagram {
            background: white;
            padding: 30px;
            border-radius: 15px;
            margin: 20px 0;
            text-align: center;
        }

        .layer-stack {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            margin: 20px 0;
        }

        .layer-block {
            width: 80%;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 10px;
            font-weight: bold;
            position: relative;
            transition: all 0.3s;
        }

        .layer-block:hover {
            transform: scale(1.05);
        }

        .layer-block.frozen {
            background: linear-gradient(135deg, #95a5a6 0%, #7f8c8d 100%);
        }

        .layer-block .lora-badge {
            position: absolute;
            right: 10px;
            top: 50%;
            transform: translateY(-50%);
            background: #2ecc71;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.8em;
            animation: glow 2s infinite;
        }

        @media (max-width: 1400px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .comparison-container {
                grid-template-columns: 1fr;
            }
        }

        .interactive-section {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 20px 0;
        }

        .parameter-efficiency {
            display: flex;
            gap: 20px;
            margin: 20px 0;
            align-items: center;
        }

        .param-visual {
            flex: 1;
            height: 60px;
            background: #e0e0e0;
            border-radius: 10px;
            position: relative;
            overflow: hidden;
        }

        .param-fill {
            height: 100%;
            background: linear-gradient(90deg, #e74c3c 0%, #c0392b 100%);
            transition: width 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .param-fill.efficient {
            background: linear-gradient(90deg, #2ecc71 0%, #27ae60 100%);
        }

        .savings-highlight {
            background: linear-gradient(135deg, #2ecc71 0%, #27ae60 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            text-align: center;
            margin: 20px 0;
        }

        .savings-highlight h3 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .savings-highlight p {
            font-size: 1.2em;
            opacity: 0.95;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üéØ LoRA Fine-Tuning Visualizer</h1>
            <p>Learn How to Adapt Large AI Models Efficiently</p>
        </div>

        <!-- Introduction Panel -->
        <div class="panel">
            <h2>üí° What is LoRA?</h2>
            
            <div class="info-box">
                <strong>LoRA = Low-Rank Adaptation</strong><br><br>
                Imagine you have a massive AI model with 7 billion parameters (weights). Traditional fine-tuning would require updating ALL 7 billion parameters - that's expensive, slow, and requires huge amounts of memory!
                <br><br>
                <strong>LoRA's brilliant idea:</strong> Instead of modifying the original weights, we add small "adapter" matrices that are much smaller. Think of it like adding a thin layer of customization on top of the base model, rather than repainting the entire thing!
            </div>

            <div class="grid-3">
                <div class="stat-card">
                    <div class="stat-label">Base Model Size</div>
                    <div class="stat-value">7B</div>
                    <p>Parameters (Frozen)</p>
                </div>
                <div class="stat-card good">
                    <div class="stat-label">LoRA Adapters</div>
                    <div class="stat-value good">4.7M</div>
                    <p>Trainable Parameters</p>
                </div>
                <div class="stat-card good">
                    <div class="stat-label">Efficiency</div>
                    <div class="stat-value good">99.93%</div>
                    <p>Fewer Parameters!</p>
                </div>
            </div>
        </div>

        <!-- Comparison Panel -->
        <div class="panel">
            <h2>‚öñÔ∏è Full Fine-Tuning vs LoRA</h2>
            
            <div class="comparison-container">
                <!-- Full Fine-Tuning -->
                <div class="method-card full-finetune">
                    <h3>üî¥ Full Fine-Tuning</h3>
                    
                    <div class="grid-2" style="gap: 15px; margin: 20px 0;">
                        <div class="stat-card bad">
                            <div class="stat-label">Parameters to Train</div>
                            <div class="stat-value bad">7B</div>
                        </div>
                        <div class="stat-card bad">
                            <div class="stat-label">Memory Required</div>
                            <div class="stat-value bad">~80 GB</div>
                        </div>
                        <div class="stat-card bad">
                            <div class="stat-label">Training Time</div>
                            <div class="stat-value bad">Days</div>
                        </div>
                        <div class="stat-card bad">
                            <div class="stat-label">Cost per Run</div>
                            <div class="stat-value bad">$$$$$</div>
                        </div>
                    </div>

                    <div class="warning-box">
                        <strong>Problems:</strong><br>
                        ‚Ä¢ Requires powerful GPU clusters<br>
                        ‚Ä¢ Very expensive to train<br>
                        ‚Ä¢ Can't store multiple versions efficiently<br>
                        ‚Ä¢ Risk of catastrophic forgetting<br>
                        ‚Ä¢ Slow iteration cycles
                    </div>
                </div>

                <!-- LoRA Fine-Tuning -->
                <div class="method-card lora">
                    <h3>üü¢ LoRA Fine-Tuning</h3>
                    
                    <div class="grid-2" style="gap: 15px; margin: 20px 0;">
                        <div class="stat-card good">
                            <div class="stat-label">Parameters to Train</div>
                            <div class="stat-value good">4.7M</div>
                        </div>
                        <div class="stat-card good">
                            <div class="stat-label">Memory Required</div>
                            <div class="stat-value good">~12 GB</div>
                        </div>
                        <div class="stat-card good">
                            <div class="stat-label">Training Time</div>
                            <div class="stat-value good">Hours</div>
                        </div>
                        <div class="stat-card good">
                            <div class="stat-label">Cost per Run</div>
                            <div class="stat-value good">$</div>
                        </div>
                    </div>

                    <div class="success-box">
                        <strong>Advantages:</strong><br>
                        ‚Ä¢ Train on consumer GPUs (even single RTX 4090!)<br>
                        ‚Ä¢ 10-100x cheaper to train<br>
                        ‚Ä¢ Store 1000s of adapters (they're tiny!)<br>
                        ‚Ä¢ Base model stays intact<br>
                        ‚Ä¢ Fast experimentation
                    </div>
                </div>
            </div>

            <div class="savings-highlight">
                <h3>üí∞ You Save 99.93% of Parameters!</h3>
                <p>Same performance, fraction of the cost</p>
            </div>
        </div>

        <!-- How LoRA Works - Mathematics -->
        <div class="panel">
            <h2>üßÆ How LoRA Works - The Mathematics</h2>
            
            <div class="info-box">
                <strong>üéØ Core Concept: Low-Rank Decomposition</strong><br><br>
                Instead of updating a huge weight matrix <strong>W</strong> (e.g., 4096√ó4096), LoRA adds two small matrices <strong>A</strong> and <strong>B</strong> whose product approximates the update.
            </div>

            <div class="formula-box">
                <div class="formula-title">Traditional Fine-Tuning:</div>
                <span class="variable">W_new</span> <span class="operator">=</span> <span class="variable">W_pretrained</span> <span class="operator">+</span> <span class="variable">ŒîW</span>
                <br><br>
                Where <span class="variable">ŒîW</span> is a FULL matrix of changes (huge!)
                <br><br>
                <div class="formula-title">LoRA Approach:</div>
                <span class="variable">W_new</span> <span class="operator">=</span> <span class="variable">W_pretrained</span> <span class="operator">+</span> <span class="variable">B</span> <span class="operator">√ó</span> <span class="variable">A</span>
                <br><br>
                Where:<br>
                ‚Ä¢ <span class="variable">W</span> = Original weight matrix (d √ó d) - <strong>FROZEN</strong><br>
                ‚Ä¢ <span class="variable">B</span> = Low-rank matrix (d √ó r) - <strong>TRAINABLE</strong><br>
                ‚Ä¢ <span class="variable">A</span> = Low-rank matrix (r √ó d) - <strong>TRAINABLE</strong><br>
                ‚Ä¢ <span class="variable">r</span> = Rank (typically 4-64, much smaller than d!)
            </div>

            <div class="matrix-visualization">
                <h3 style="color: #0f3460; margin-bottom: 20px;">Visual Decomposition</h3>
                
                <div class="matrix-container">
                    <!-- Original Matrix W -->
                    <div class="matrix">
                        <div class="matrix-label">W (Frozen)<br>4096 √ó 4096</div>
                        <div class="matrix-grid" style="grid-template-columns: repeat(8, 30px);">
                            <!-- Create 8x8 visual representation -->
                            <script>
                                for (let i = 0; i < 64; i++) {
                                    document.write('<div class="matrix-cell frozen"></div>');
                                }
                            </script>
                        </div>
                        <div class="matrix-dimensions">16.8M parameters<br>‚ùÑÔ∏è Frozen</div>
                    </div>

                    <div class="operator">+</div>

                    <!-- Matrix B -->
                    <div class="matrix">
                        <div class="matrix-label">B<br>4096 √ó 8</div>
                        <div class="matrix-grid" style="grid-template-columns: repeat(8, 30px);">
                            <script>
                                for (let i = 0; i < 64; i++) {
                                    document.write('<div class="matrix-cell lora-adapter"></div>');
                                }
                            </script>
                        </div>
                        <div class="matrix-dimensions">32,768 params<br>üî• Trainable</div>
                    </div>

                    <div class="operator">√ó</div>

                    <!-- Matrix A -->
                    <div class="matrix">
                        <div class="matrix-label">A<br>8 √ó 4096</div>
                        <div class="matrix-grid" style="grid-template-columns: repeat(8, 30px);">
                            <script>
                                for (let i = 0; i < 64; i++) {
                                    document.write('<div class="matrix-cell lora-adapter"></div>');
                                }
                            </script>
                        </div>
                        <div class="matrix-dimensions">32,768 params<br>üî• Trainable</div>
                    </div>

                    <div class="equals">=</div>

                    <!-- Result -->
                    <div class="matrix">
                        <div class="matrix-label">W_adapted<br>4096 √ó 4096</div>
                        <div class="matrix-grid" style="grid-template-columns: repeat(8, 30px);">
                            <script>
                                for (let i = 0; i < 64; i++) {
                                    document.write('<div class="matrix-cell trainable"></div>');
                                }
                            </script>
                        </div>
                        <div class="matrix-dimensions">Adapted weights<br>‚ú® Combined</div>
                    </div>
                </div>

                <div class="success-box">
                    <strong>üí° The Magic:</strong><br>
                    Instead of training 16.8M parameters, we only train 65,536 parameters (B + A)!<br>
                    That's <strong>256√ó fewer parameters</strong> for this one layer alone!
                </div>
            </div>
        </div>

        <!-- Interactive Rank Selector -->
        <div class="panel">
            <h2>üéõÔ∏è Interactive: Adjust LoRA Rank</h2>
            
            <div class="info-box">
                <strong>What is Rank?</strong><br>
                The rank (r) determines the "capacity" of the LoRA adapters. Higher rank = more parameters = more expressive, but also more expensive.
            </div>

            <div class="interactive-section">
                <div class="rank-selector">
                    <label>Rank (r): <span id="rankValue" class="rank-value">8</span></label>
                    <input type="range" class="rank-slider" id="rankSlider" min="1" max="128" value="8" oninput="updateRank()">
                </div>

                <div class="grid-2" style="margin-top: 30px;">
                    <div class="stat-card">
                        <div class="stat-label">LoRA Parameters (One Layer)</div>
                        <div class="stat-value" id="loraParams">65,536</div>
                        <p id="loraVsBase">0.39% of base layer</p>
                    </div>
                    <div class="stat-card">
                        <div class="stat-label">Total LoRA Parameters (All Layers)</div>
                        <div class="stat-value" id="totalLoraParams">4.7M</div>
                        <p id="totalVsBase">0.067% of 7B model</p>
                    </div>
                </div>

                <div class="parameter-efficiency">
                    <div>
                        <strong>Base Model:</strong><br>
                        <div class="param-visual">
                            <div class="param-fill" style="width: 100%;">7 Billion Parameters</div>
                        </div>
                    </div>
                </div>

                <div class="parameter-efficiency">
                    <div>
                        <strong>LoRA Adapters:</strong><br>
                        <div class="param-visual">
                            <div class="param-fill efficient" id="loraVisual" style="width: 0.067%;">
                                <span id="loraVisualText">4.7M</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="warning-box">
                    <strong>üí° Choosing Rank:</strong><br>
                    ‚Ä¢ <strong>r = 1-4:</strong> Extremely efficient, good for simple adaptations<br>
                    ‚Ä¢ <strong>r = 8-16:</strong> Sweet spot for most tasks (default)<br>
                    ‚Ä¢ <strong>r = 32-64:</strong> More capacity, for complex adaptations<br>
                    ‚Ä¢ <strong>r = 128+:</strong> High capacity, but loses efficiency benefits
                </div>
            </div>
        </div>

        <!-- Training Simulation -->
        <div class="panel">
            <h2>üöÄ Simulate LoRA Training</h2>
            
            <div class="grid-2">
                <div>
                    <h3 style="color: #0f3460; margin-bottom: 15px;">Training Configuration</h3>
                    
                    <div class="info-box">
                        <strong>Task:</strong> Fine-tune a 7B model for medical question answering<br>
                        <strong>Dataset:</strong> 10,000 medical Q&A pairs<br>
                        <strong>Method:</strong> LoRA (rank = 8)<br>
                        <strong>GPU:</strong> Single NVIDIA RTX 4090 (24GB)
                    </div>

                    <div style="text-align: center; margin: 20px 0;">
                        <button class="btn-primary" onclick="startTraining()" id="trainBtn">
                            ‚ñ∂Ô∏è Start LoRA Training
                        </button>
                        <button class="btn-secondary" onclick="resetTraining()" id="resetBtn">
                            üîÑ Reset
                        </button>
                    </div>
                </div>

                <div>
                    <h3 style="color: #0f3460; margin-bottom: 15px;">Training Metrics</h3>
                    
                    <div class="training-progress">
                        <div class="progress-label">
                            <span>Training Progress</span>
                            <span id="progressPercent">0%</span>
                        </div>
                        <div class="progress-bar-container">
                            <div class="progress-bar" id="progressBar" style="width: 0%;">Ready</div>
                        </div>
                    </div>

                    <div class="grid-2" style="gap: 15px;">
                        <div class="stat-card">
                            <div class="stat-label">Epoch</div>
                            <div class="stat-value" id="epochValue">0/3</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">Loss</div>
                            <div class="stat-value" id="lossValue">-</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">Training Time</div>
                            <div class="stat-value" id="timeValue">0s</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-label">GPU Memory</div>
                            <div class="stat-value" id="memoryValue">0 GB</div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="training-log" id="trainingLog">
                <div class="log-entry" style="color: #ffd700;">>> Ready to start training...</div>
            </div>
        </div>

        <!-- Architecture Visualization -->
        <div class="panel">
            <h2>üèóÔ∏è Model Architecture with LoRA</h2>
            
            <div class="architecture-diagram">
                <h3 style="color: #0f3460; margin-bottom: 20px;">Transformer Model with LoRA Adapters</h3>
                
                <div class="layer-stack">
                    <div class="layer-block">
                        Input Embedding Layer
                        <span class="lora-badge">+ LoRA</span>
                    </div>
                    
                    <div style="font-size: 1.5em; color: #0f3460;">‚Üì</div>
                    
                    <div class="layer-block frozen">
                        Transformer Block 1 (Frozen)
                        <span class="lora-badge">+ LoRA</span>
                    </div>
                    
                    <div style="font-size: 1.5em; color: #0f3460;">‚Üì</div>
                    
                    <div class="layer-block frozen">
                        Transformer Block 2 (Frozen)
                        <span class="lora-badge">+ LoRA</span>
                    </div>
                    
                    <div style="font-size: 1.5em; color: #0f3460;">‚Üì</div>
                    
                    <div class="layer-block frozen">
                        ... 30 more blocks (Frozen)
                        <span class="lora-badge">+ LoRA</span>
                    </div>
                    
                    <div style="font-size: 1.5em; color: #0f3460;">‚Üì</div>
                    
                    <div class="layer-block">
                        Output Head
                        <span class="lora-badge">+ LoRA</span>
                    </div>
                </div>

                <div class="info-box" style="text-align: left;">
                    <strong>Key Points:</strong><br>
                    ‚Ä¢ <span style="color: #95a5a6;">‚ñ† Gray blocks</span> = Frozen original weights (7B parameters)<br>
                    ‚Ä¢ <span style="color: #2ecc71;">‚ñ† Green badges</span> = LoRA adapters (4.7M parameters)<br>
                    ‚Ä¢ LoRA adapters are added to attention layers (Q, K, V, O projections)<br>
                    ‚Ä¢ You can even have multiple LoRA adapters and swap them out!
                </div>
            </div>
        </div>

        <!-- Use Cases -->
        <div class="panel">
            <h2>üéØ Real-World Use Cases</h2>
            
            <div class="use-case-grid">
                <div class="use-case-card">
                    <h4>üí¨ Custom Chatbots</h4>
                    <p>Adapt a general model to your company's tone, terminology, and knowledge base without training from scratch.</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> Customer service bot for banking
                    </div>
                </div>

                <div class="use-case-card">
                    <h4>üè• Domain-Specific Models</h4>
                    <p>Create medical, legal, or scientific AI assistants by fine-tuning on specialized datasets.</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> Medical diagnosis assistant
                    </div>
                </div>

                <div class="use-case-card">
                    <h4>üåç Language Adaptation</h4>
                    <p>Adapt an English model to other languages or dialects efficiently.</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> Regional language support
                    </div>
                </div>

                <div class="use-case-card">
                    <h4>üé® Style Transfer</h4>
                    <p>Make AI write in specific styles - formal, casual, poetic, technical, etc.</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> Shakespearean text generator
                    </div>
                </div>

                <div class="use-case-card">
                    <h4>üìä Task-Specific Fine-Tuning</h4>
                    <p>Optimize for specific tasks like summarization, translation, or code generation.</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> SQL query generator
                    </div>
                </div>

                <div class="use-case-card">
                    <h4>üîÑ Multi-Tenant Systems</h4>
                    <p>One base model, multiple LoRA adapters for different customers. Swap adapters per request!</p>
                    <div style="margin-top: 15px; padding: 10px; background: #f0f0f0; border-radius: 6px;">
                        <strong>Example:</strong> SaaS AI platform
                    </div>
                </div>
            </div>
        </div>

        <!-- Comparison Table -->
        <div class="panel">
            <h2>üìä Complete Comparison Table</h2>
            
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Full Fine-Tuning</th>
                        <th>LoRA</th>
                        <th>Winner</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Parameters Trained</strong></td>
                        <td class="bad">7,000,000,000</td>
                        <td class="good">4,700,000</td>
                        <td class="good">LoRA (99.93% less)</td>
                    </tr>
                    <tr>
                        <td><strong>Memory Required</strong></td>
                        <td class="bad">80+ GB</td>
                        <td class="good">12-24 GB</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Training Time (1 epoch)</strong></td>
                        <td class="bad">24+ hours</td>
                        <td class="good">2-4 hours</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Training Cost</strong></td>
                        <td class="bad">$5,000+</td>
                        <td class="good">$50-100</td>
                        <td class="good">LoRA (100√ó cheaper)</td>
                    </tr>
                    <tr>
                        <td><strong>Storage per Adapter</strong></td>
                        <td class="bad">28 GB</td>
                        <td class="good">20-100 MB</td>
                        <td class="good">LoRA (280√ó smaller)</td>
                    </tr>
                    <tr>
                        <td><strong>GPU Requirements</strong></td>
                        <td class="bad">A100 cluster</td>
                        <td class="good">Single RTX 4090</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Risk of Forgetting</strong></td>
                        <td class="bad">High</td>
                        <td class="good">Low (base frozen)</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Iteration Speed</strong></td>
                        <td class="bad">Slow</td>
                        <td class="good">Fast</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Multiple Adapters</strong></td>
                        <td class="bad">Need full copies</td>
                        <td class="good">Swap on demand</td>
                        <td class="good">LoRA</td>
                    </tr>
                    <tr>
                        <td><strong>Performance</strong></td>
                        <td class="good">Excellent</td>
                        <td class="good">Comparable</td>
                        <td>Tie</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- Key Takeaways -->
        <div class="panel">
            <h2>üéì Key Takeaways</h2>
            
            <div class="grid-2">
                <div class="success-box">
                    <strong>‚úÖ Why LoRA is Revolutionary:</strong><br><br>
                    1. <strong>Democratizes AI:</strong> Anyone can fine-tune large models on consumer hardware<br><br>
                    2. <strong>Cost Effective:</strong> 100√ó cheaper than full fine-tuning<br><br>
                    3. <strong>Storage Efficient:</strong> Store 1000s of adapters vs few full models<br><br>
                    4. <strong>Fast Iteration:</strong> Experiment quickly without breaking the bank<br><br>
                    5. <strong>Safe:</strong> Base model stays intact, easy to revert<br><br>
                    6. <strong>Flexible:</strong> Swap adapters on-the-fly for different tasks
                </div>

                <div class="info-box">
                    <strong>üß† The Core Insight:</strong><br><br>
                    Not all parameters are equally important! Most of the adaptation can be captured by a low-rank update, which requires far fewer parameters.
                    <br><br>
                    This is based on the hypothesis that learned weight updates have low "intrinsic rank" - meaning the changes can be represented efficiently in a lower-dimensional space.
                    <br><br>
                    <strong>Think of it like:</strong> Instead of rewriting an entire book, you just add sticky notes with corrections and additions!
                </div>
            </div>

            <div class="warning-box">
                <strong>‚ö†Ô∏è When NOT to Use LoRA:</strong><br><br>
                ‚Ä¢ When you need to completely change the model's knowledge (use full fine-tuning)<br>
                ‚Ä¢ When you have unlimited budget and compute (full fine-tuning might be slightly better)<br>
                ‚Ä¢ For extremely small models (the overhead isn't worth it)<br>
                ‚Ä¢ When you need to modify architectural components (LoRA only works on weight matrices)
            </div>
        </div>
    </div>

    <script>
        let isTraining = false;
        let trainingInterval;

        function updateRank() {
            const rank = parseInt(document.getElementById('rankSlider').value);
            document.getElementById('rankValue').textContent = rank;

            const d = 4096; // dimension
            const loraParamsOneLayer = d * rank * 2; // B and A matrices
            const numLayers = 32; // typical for 7B model
            const totalLoraParams = loraParamsOneLayer * numLayers;
            const baseParams = 7000000000;
            const baseLayerParams = 16777216; // d * d

            // Update displays
            document.getElementById('loraParams').textContent = loraParamsOneLayer.toLocaleString();
            document.getElementById('loraVsBase').textContent = 
                ((loraParamsOneLayer / baseLayerParams) * 100).toFixed(3) + '% of base layer';

            document.getElementById('totalLoraParams').textContent = 
                (totalLoraParams / 1000000).toFixed(1) + 'M';
            document.getElementById('totalVsBase').textContent = 
                ((totalLoraParams / baseParams) * 100).toFixed(3) + '% of 7B model';

            // Update visual
            const percentage = (totalLoraParams / baseParams) * 100;
            const loraVisual = document.getElementById('loraVisual');
            loraVisual.style.width = Math.max(0.1, percentage) + '%'; // Minimum visible width
            document.getElementById('loraVisualText').textContent = 
                (totalLoraParams / 1000000).toFixed(1) + 'M';
        }

        async function startTraining() {
            if (isTraining) return;
            
            isTraining = true;
            document.getElementById('trainBtn').disabled = true;
            
            const log = document.getElementById('trainingLog');
            log.innerHTML = '';
            
            addLog('üöÄ Initializing LoRA training...', 'ffd700');
            await sleep(500);
            addLog('üì¶ Loading base model (7B parameters)...', '00ffff');
            await sleep(1000);
            addLog('‚úÖ Base model loaded. Freezing all weights...', '00ff00');
            await sleep(500);
            addLog('üîß Initializing LoRA adapters (rank=8, 4.7M parameters)...', '00ffff');
            await sleep(800);
            addLog('‚úÖ LoRA adapters initialized', '00ff00');
            await sleep(300);
            addLog('üìä Loading medical Q&A dataset (10,000 examples)...', '00ffff');
            await sleep(1000);
            addLog('‚úÖ Dataset loaded and preprocessed', '00ff00');
            await sleep(500);
            addLog('', '00ff00');
            addLog('üéØ Starting training (3 epochs)...', 'ffd700');
            await sleep(500);

            const startTime = Date.now();
            const totalSteps = 3 * 100; // 3 epochs, 100 steps each
            let currentStep = 0;

            for (let epoch = 1; epoch <= 3; epoch++) {
                addLog(`\nüìà Epoch ${epoch}/3`, 'ffd700');
                
                for (let step = 1; step <= 100; step++) {
                    currentStep++;
                    const progress = (currentStep / totalSteps) * 100;
                    const loss = 2.5 - (currentStep / totalSteps) * 2.0 + Math.random() * 0.1;
                    const elapsedTime = Math.floor((Date.now() - startTime) / 1000);
                    const memoryUsage = 12 + Math.random() * 2;

                    // Update progress bar
                    document.getElementById('progressBar').style.width = progress + '%';
                    document.getElementById('progressBar').textContent = `${Math.round(progress)}%`;
                    document.getElementById('progressPercent').textContent = `${Math.round(progress)}%`;

                    // Update metrics
                    document.getElementById('epochValue').textContent = `${epoch}/3`;
                    document.getElementById('lossValue').textContent = loss.toFixed(4);
                    document.getElementById('timeValue').textContent = elapsedTime + 's';
                    document.getElementById('memoryValue').textContent = memoryUsage.toFixed(1) + ' GB';

                    if (step % 25 === 0) {
                        addLog(`  Step ${step}/100: Loss = ${loss.toFixed(4)}, GPU = ${memoryUsage.toFixed(1)}GB`);
                    }

                    await sleep(50);
                }
                
                const avgLoss = (2.5 - (epoch / 3) * 2.0).toFixed(4);
                addLog(`‚úÖ Epoch ${epoch} complete. Avg Loss: ${avgLoss}`, '00ff00');
            }

            addLog('', '00ff00');
            addLog('üéâ Training completed successfully!', '00ff00');
            addLog('üíæ Saving LoRA adapters...', '00ffff');
            await sleep(1000);
            addLog('‚úÖ Adapters saved (size: 18.8 MB)', '00ff00');
            await sleep(500);
            addLog('', '00ff00');
            addLog('üìä Final Training Stats:', 'ffd700');
            addLog(`   ‚Ä¢ Total time: ${Math.floor((Date.now() - startTime) / 1000)}s`, '00ffff');
            addLog(`   ‚Ä¢ Final loss: 0.5234`, '00ffff');
            addLog(`   ‚Ä¢ Peak GPU memory: 13.2 GB`, '00ffff');
            addLog(`   ‚Ä¢ Trainable parameters: 4,718,592`, '00ffff');
            addLog(`   ‚Ä¢ Cost estimate: ~$3.50`, '00ffff');
            await sleep(500);
            addLog('', '00ff00');
            addLog('‚ú® Your model is now fine-tuned for medical Q&A!', '00ff00');

            isTraining = false;
            document.getElementById('trainBtn').disabled = false;
        }

        function resetTraining() {
            if (isTraining) {
                clearInterval(trainingInterval);
                isTraining = false;
            }

            document.getElementById('progressBar').style.width = '0%';
            document.getElementById('progressBar').textContent = 'Ready';
            document.getElementById('progressPercent').textContent = '0%';
            document.getElementById('epochValue').textContent = '0/3';
            document.getElementById('lossValue').textContent = '-';
            document.getElementById('timeValue').textContent = '0s';
            document.getElementById('memoryValue').textContent = '0 GB';
            document.getElementById('trainBtn').disabled = false;

            const log = document.getElementById('trainingLog');
            log.innerHTML = '<div class="log-entry" style="color: #ffd700;">>> Ready to start training...</div>';
        }

        function addLog(message, color = '00ff00') {
            const log = document.getElementById('trainingLog');
            const entry = document.createElement('div');
            entry.className = 'log-entry';
            entry.style.color = '#' + color;
            entry.textContent = message;
            log.appendChild(entry);
            log.scrollTop = log.scrollHeight;
        }

        function sleep(ms) {
            return new Promise(resolve => setTimeout(resolve, ms));
        }

        // Initialize
        window.onload = function() {
            updateRank();
        };
    </script>
</body>
</html>
